name: CI Pipeline

on:
  push:
    branches:
      # by adding the branches, we can trigger the github action
      - main
      - 'coding/**' # using / and * requires ""
  pull_request:
    branches:
      - main

env:
  PROJECT_ID: tlp-niche-market-research
  REGION: us-central1
  REPO_NAME: niche-market-product
  IMAGE_NAME: niche-market-product

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    # These steps are created as every job requires to run the code successfully.
    # Ex. Add tasks outside of main.py so we can execute the code such as installing the packages.
    # then no need to add tasks like 'read data' since it is a part of main.py
    # Other possible tasks can be testing, deployment, quality control etc.
    # If added more python files like read.py write.py that are run in main.py, we don't need to add these files in the steps.

    steps:
      # ─── CI STEPS ─────────────────────────────────────────
      - name: Checkout code # CI: get the latest code
        uses: actions/checkout@v4

      - name: Set up Python # CI: prepare Python runtime
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install dependencies # CI: install libs for pipeline
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Authenticate to Google Cloud # CI: allow GCS uploads
        uses: google-github-actions/auth@v2
        with:
          credentials_json: '${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}'

      - name: Set up gcloud CLI # CI: for gcloud commands
        uses: google-github-actions/setup-gcloud@v1

#      # Produce the output in GCS when the github action is running
       # however, it is not a best practice to store a new data in CI pipeline. It also costs more for API and Storage.
       # CI is for deploying code, not producing data. Data pipeline is usually running on schedules against evolving data
       # need their own monitoring and failture handling.
#      - name: Run data pipeline # CI: run main.py → process data & upload to GCS
#        run: |
#          python main.py

      # ─── CD STEPS ─────────────────────────────────────────
      - name: Configure Docker to use Artifact Registry  # CD: prepare Docker push
        run: |
          gcloud auth configure-docker $REGION-docker.pkg.dev

      - name: Build Docker image # CD: containerize your service (server.py)
        run: |
          docker build -t $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/$IMAGE_NAME:latest .

      - name: Push Docker image to Artifact Registry # CD: publish to Artifact Registry
        run: |
          docker push $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/$IMAGE_NAME:latest

      - name: Deploy to Cloud Run from image # CD: update live service
        run: |
          gcloud run deploy $IMAGE_NAME \
            --image $REGION-docker.pkg.dev/$PROJECT_ID/$REPO_NAME/$IMAGE_NAME:latest \
            --platform managed \
            --region $REGION \
            --allow-unauthenticated